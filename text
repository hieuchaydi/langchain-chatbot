# gemini_client.py – BẢN CUỐI CÙNG, HOÀN HẢO TUYỆT ĐỐI 2025 (FIXED 100%)
import os
from dotenv import load_dotenv
import google.generativeai as genai

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("Thiếu GEMINI_API_KEY trong .env")

genai.configure(api_key=API_KEY)

# Model Gemini chính
model = genai.GenerativeModel(
    model_name="gemini-2.5-flash",          
    safety_settings=[
        {"category": c, "threshold": "BLOCK_NONE"}
        for c in [
            "HARM_CATEGORY_HARASSMENT",
            "HARM_CATEGORY_HATE_SPEECH",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "HARM_CATEGORY_DANGEROUS_CONTENT",
        ]
    ],
    generation_config={
        "temperature": 0.2,
        "max_output_tokens": 800,
    },
)

_reranker_model = None


def chat_with_gemini(user_question: str, context_chunks: list) -> str:
    if not context_chunks:
        return "Hiện tại chưa có thông tin này trong tài liệu."

    global _reranker_model
    if _reranker_model is None:
        try:
            from sentence_transformers import SentenceTransformer
            _reranker_model = SentenceTransformer("BAAI/bge-reranker-v2-m3")
            print("Đã load reranker BAAI/bge-reranker-v2-m3 – thần thánh 2025")
        except Exception as e:
            print(f"[RERANKER ERROR] {e} → dùng fallback không rerank")
            _reranker_model = False

    # Chuẩn bị dữ liệu để rerank
    candidates = []
    for doc, meta in context_chunks:
        title = meta.get("title", "Không có tiêu đề")
        candidates.append((title, doc.strip(), meta))

    # Nếu có reranker → dùng nó cực chuẩn
    if _reranker_model:
        try:
            from sentence_transformers import util
            import torch

            # GHÉP TITLE + NỘI DUNG ĐỂ RERANK SIÊU CHUẨN (đây là chỗ cũ bị sai chết người)
            candidate_texts = [f"{title}\n{content}" for title, content, _ in candidates]

            pairs = [[user_question, text] for text in candidate_texts]
            scores = _reranker_model.predict(pairs)

            if torch.is_tensor(scores):
                scores = scores.cpu().tolist()

            # Sắp xếp lại theo score reranker
            ranked = sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)
            top_candidates = [cand for _, cand in ranked[:12]]  # lấy dư chút cho chắc
        except Exception as e:
            print(f"[RERANK FAIL] {e} → fallback thứ tự gốc")
            top_candidates = candidates
    else:
        top_candidates = candidates

    # Tạo context cuối cùng – ưu tiên title trùng hoặc giống nhất lên đầu
    context_parts = []
    seen_titles = set()
    for title, content, meta in top_candidates:
        if title not in seen_titles:
            context_parts.append(f"### {title}\n{content}")
            seen_titles.add(title)
            if len(context_parts) >= 8:  # đủ rồi, không để quá dài
                break

    context = "\n\n".join(context_parts)
    if not context.strip():
        return "Hiện tại chưa có thông tin này trong tài liệu."

    # PROMPT HOÀN HẢO NHẤT 2025 – vừa nghiêm khắc vừa không bị từ chối nhầm
    prompt = f"""Bạn là trợ lý tìm kiếm thông tin cực kỳ chính xác.
Dựa HOÀN TOÀN vào dữ liệu dưới đây để trả lời câu hỏi.
Nếu có dù chỉ 1 câu liên quan thì phải trả lời dựa vào đó.
Chỉ được phép nói "Hiện tại chưa có thông tin này trong tài liệu." khi thật sự KHÔNG có bất kỳ thông tin nào liên quan.

DỮ LIỆU:
{context}

CÂU HỎI: {user_question}

Trả lời ngắn gọn, tự nhiên bằng tiếng Việt, không thêm thắt, không giải thích nguồn:"""

    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.0,
                "max_output_tokens": 600,
            }
        )
        text = response.text.strip()
        # Nếu Gemini nhát quá vẫn trả về rỗng hoặc quá ngắn → fallback an toàn
        if len(text) < 10 or "không có thông tin" in text.lower():
            return "Hiện tại chưa có thông tin này trong tài liệu."
        return text
    except Exception as e:
        print(f"[Gemini Error] {e}")
        return "Hiện tại chưa có thông tin này trong tài liệu."




# vectorstore.py – BẢN HOÀN CHỈNH + NOTE SIÊU CHI TIẾT 2025
# Chức năng: Quản lý ChromaDB + Hybrid Search (Vector + BM25) + Chunking thông minh theo Heading

import chromadb
from chromadb.utils import embedding_functions
from pathlib import Path
from rank_bm25 import BM25Okapi
from typing import List, Tuple
import re  # ← QUAN TRỌNG: Dùng để làm sạch title khi so sánh chính xác

# ===================================================================
# 1. CÀI ĐẶT CHROMA + EMBEDDING MODEL
# ===================================================================
ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
# → Model nhẹ, nhanh, chính xác cao cho tiếng Anh + tiếng Việt ổn
client = chromadb.PersistentClient(path="./vectorstore")
# → Lưu database vào thư mục ./vectorstore (dễ backup, dễ xóa)

def get_collection():
    """Lấy hoặc tạo collection 'markdown_docs' với cosine similarity"""
    return client.get_or_create_collection(
        name="markdown_docs",
        embedding_function=ef,
        metadata={"hnsw:space": "cosine"}  # ← Tốt nhất cho embedding
    )

# ===================================================================
# 2. BM25 – TÌM KIẾM THEO TỪ KHÓA (rất mạnh với tiếng Việt + tên riêng)
# ===================================================================
_bm25: BM25Okapi | None = None                    # Model BM25 toàn cục
_all_docs_tokenized: List[List[str]] = []          # Danh sách tài liệu đã được tokenize
_all_ids: List[str] = []                           # Danh sách ID tương ứng

def _rebuild_bm25() -> None:
    """
    TÁI TẠO BM25 SAU KHI THÊM/XÓA TÀI LIỆU
    → Đảm bảo tìm kiếm keyword luôn chính xác 100%
    """
    global _bm25, _all_docs_tokenized, _all_ids
    coll = get_collection()
    data = coll.get(include=["documents", "metadatas"])
    docs = data.get("documents", [])
    _all_ids = data.get("ids", [])
    _all_docs_tokenized = [doc.lower().split() for doc in docs]  # Tokenize đơn giản

    if _all_docs_tokenized:
        _bm25 = BM25Okapi(_all_docs_tokenized)  # Tạo BM25 index
        print(f"BM25 đã được rebuild – có {len(_all_docs_tokenized)} tài liệu")
    else:
        _bm25 = None
        print("BM25: Không có tài liệu nào")

# Gọi lần đầu khi khởi động
_rebuild_bm25()

# ===================================================================
# 3. THÊM TÀI LIỆU – CHUNKING THÔNG MINH THEO HEADING
# ===================================================================
def add_documents(file_content: str, filename: str) -> None:
    """
    Thêm file Markdown vào vector store
    - Tự động chia chunk theo heading (#, ##, ###)
    - Tự động xóa dữ liệu cũ của file này trước khi thêm mới (tránh trùng)
    - Tạo title_clean để so sánh chính xác khi lọc trùng tiêu đề
    """
    coll = get_collection()
    filename_clean = Path(filename).name  # Chỉ lấy tên file

    # BƯỚC 1: XÓA DỮ LIỆU CŨ CỦA FILE NÀY (nếu có)
    coll.delete(where={"source": filename_clean})
    print(f"Đã xóa dữ liệu cũ của file: {filename_clean}")

    lines = file_content.split("\n")
    current_chunk: List[str] = []
    current_title = "Không có tiêu đề"
    chunks = []

    # BƯỚC 2: CHIA CHUNK THEO HEADING (# đến ####)
    for line in lines:
        line = line.rstrip()
        if line.startswith("#"):
            hash_count = len(line) - len(line.lstrip("#"))
            stripped = line.lstrip("# ").strip()

            if 1 <= hash_count <= 4:  # Chỉ lấy heading cấp 1-4
                # Lưu chunk cũ trước khi chuyển heading
                if current_chunk:
                    text = "\n".join(current_chunk).strip()
                    if text:
                        word_count = len(text.split())
                        level = 5 if word_count > 800 else 4 if word_count > 500 else 3
                        chunks.append({"text": text, "title": current_title, "level": level})
                    current_chunk = []

                # Bắt đầu chunk mới với heading mới
                current_title = stripped if stripped else f"Heading cấp {hash_count}"
                current_chunk.append(line)
                continue

        current_chunk.append(line)

    # Lưu chunk cuối cùng
    if current_chunk:
        text = "\n".join(current_chunk).strip()
        if text:
            word_count = len(text.split())
            level = 5 if word_count > 800 else 4 if word_count > 500 else 3
            chunks.append({"text": text, "title": current_title, "level": level})

    # BƯỚC 3: NẾU KHÔNG CÓ HEADING → CHIA THEO ĐỘ DÀI (600 từ/chunk, overlap 100)
    if not chunks:
        print("File không có heading → chia chunk theo độ dài")
        words = file_content.split()
        step = 600
        overlap = 100
        i = 0
        while i < len(words):
            chunk_words = words[i:i + step]
            if not chunk_words:
                break
            text = " ".join(chunk_words)
            word_count = len(chunk_words)
            level = 5 if word_count > 800 else 4 if word_count > 500 else 3
            chunks.append({"text": text, "title": "Không có tiêu đề", "level": level})
            i += step - overlap

    # BƯỚC 4: THÊM VÀO CHROMA VỚI METADATA SIÊU THÔNG MINH
    if chunks:
        docs = [c["text"] for c in chunks]
        metadatas = []
        ids = []

        for i, c in enumerate(chunks):
            # TẠO title_clean: loại bỏ dấu câu, xuống thường, chuẩn hóa khoảng trắng
            # → Dùng để lọc trùng tiêu đề chính xác 100%
            clean_title = re.sub(r'[^\w\s]', '', c["title"].lower())
            clean_title = re.sub(r'\s+', ' ', clean_title).strip()

            metadatas.append({
                "source": filename_clean,
                "title": c["title"],           # Giữ nguyên để hiển thị đẹp
                "title_clean": clean_title,    # ← SIÊU QUAN TRỌNG: dùng để tránh trùng tiêu đề
                "level": c["level"]
            })
            ids.append(f"{filename_clean}_{i}")

        coll.add(documents=docs, metadatas=metadatas, ids=ids)
        print(f"Đã thêm {len(chunks)} chunk từ file: {filename_clean}")

    # BƯỚC 5: CẬP NHẬT LẠI BM25
    _rebuild_bm25()

# ===================================================================
# 4. TÌM KIẾM HYBRID SIÊU MẠNH (Vector + BM25)
# ===================================================================
def query_documents(query: str, n_results: int = 15) -> Tuple[List[str], List[dict]]:
    """
    Tìm kiếm kết hợp Vector Search + BM25 → kết quả cực kỳ chính xác
    """
    coll = get_collection()

    # BƯỚC 1: Vector Search (tìm ngữ nghĩa)
    vec_result = coll.query(query_texts=[query], n_results=n_results * 2)
    vec_docs = vec_result['documents'][0]
    vec_metas = vec_result['metadatas'][0]
    vec_distances = vec_result['distances'][0]
    vec_ids = vec_result['ids'][0]

    # Nếu không có BM25 → trả về kết quả vector
    if not _bm25 or not _all_ids:
        print("Chỉ dùng Vector Search (BM25 chưa sẵn sàng)")
        return vec_docs[:n_results], vec_metas[:n_results]

    # BƯỚC 2: BM25 Search (tìm từ khóa chính xác)
    tokenized_query = query.lower().split()
    bm25_scores = _bm25.get_scores(tokenized_query)

    # BƯỚC 3: KẾT HỢP ĐIỂM (Hybrid Scoring)
    score_dict = {}
    for i, doc_id in enumerate(_all_ids):
        score_dict[doc_id] = bm25_scores[i]  # Điểm BM25

    # Cộng thêm điểm từ Vector (càng gần → điểm càng cao)
    for vid, dist in zip(vec_ids, vec_distances):
        if vid in score_dict:
            score_dict[vid] += (1.0 - dist) * 20  # Nhân 20 để Vector có trọng số mạnh

    # Sắp xếp lại theo tổng điểm
    sorted_scores = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
    top_ids = [sid for sid, _ in sorted_scores[:n_results]]

    # Lấy dữ liệu thật
    final = coll.get(ids=top_ids, include=["documents", "metadatas"])
    print(f"Hybrid Search hoàn tất – trả về {len(final['documents'])} kết quả")
    return final["documents"], final["metadatas"]

# ===================================================================
# 5. CÁC HÀM HỖ TRỢ
# ===================================================================
def get_all_documents():
    """Lấy toàn bộ tài liệu (dùng để debug hoặc hiển thị danh sách)"""
    coll = get_collection()
    data = coll.get(include=["documents", "metadatas"])
    if "ids" not in data or not data["ids"]:
        data["ids"] = [f"unknown_{i}" for i in range(len(data["documents"]))]
    return data

def reset_vectorstore():
    """XÓA SẠCH TOÀN BỘ VECTOR STORE – DÙNG KHI MUỐN LÀM LẠI TỪ ĐẦU"""
    global _bm25, _all_docs_tokenized, _all_ids
    try:
        client.delete_collection("markdown_docs")
        print("Đã xóa collection markdown_docs")
    except:
        print("Không tìm thấy collection để xóa")
    _bm25 = None
    _all_docs_tokenized = []
    _all_ids = []
    _rebuild_bm25()
    print("Vector store đã được reset hoàn toàn!")

# Gọi lại khi khởi động (phòng trường hợp DB bị lỗi)
_rebuild_bm25()


INFO:     127.0.0.1:63095 - "POST /chat HTTP/1.1" 200 OK
[Gemini Intent Error] Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.
QUERY | 0.038s | 34 results | Cache: 0/2
Nếu đoạn dưới heading quá dài (>800 từ) sẽ tự động tách thêm chunk nhưng vẫn giữ chung title.