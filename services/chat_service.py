from typing import Dict, Any, List, Optional
"""
Chat Service Module

ChatService v4.5.0 (Token-Optimized CSKH + Summary Memory)
- KhÃ´ng gá»i Gemini cho:
  - social
  - small talk
  - deny
  - product inquiry
  - quick reply
- Gemini chá»‰ lÃ  fallback classifier
- Giá»¯ nguyÃªn:
  - CSKH routing
  - Hidemium priority
  - query expansion
  - multi-language
  - deny + escalation
- Bá»• sung:
  - Summary há»™i thoáº¡i cross-session
  - Auto summarize
  - Inject summary vÃ o RAG
"""

import asyncio
import logging
import os
import re

from services.intent_registry import intent_registry
from config.quick_reply import QuickReplyHandler
from models.vector_store import VectorStoreManager, SESSION_MEMORY
from models.db import (
    save_message,
    save_conversation_summary,
    load_latest_summary,
    load_messages
)
from middleware.badword_filter import contains_swear, get_swear_response
from models.gemini_analyzer import analyze_question, translate_text


LOG_DIR = "log"
os.makedirs(LOG_DIR, exist_ok=True)

pipeline_logger = logging.getLogger("chat_pipeline")
pipeline_logger.setLevel(logging.INFO)
pipeline_logger.propagate = False

if not pipeline_logger.handlers:
    handler = logging.FileHandler(
        os.path.join(LOG_DIR, "chat_pipeline.log"),
        encoding="utf-8"
    )
    formatter = logging.Formatter(
        "%(asctime)s | %(levelname)s | %(message)s"
    )
    handler.setFormatter(formatter)
    pipeline_logger.addHandler(handler)


def log_flow(step: str, data: Optional[Dict[str, Any]] = None):
    if data:
        pipeline_logger.info(f"[FLOW] {step} | {data}")
    else:
        pipeline_logger.info(f"[FLOW] {step}")


SUMMARY_MIN_TURNS = 10


async def summarize_session(session_id: str) -> str:
    messages = load_messages(session_id, limit=50)
    if not messages:
        return ""

    text = "\n".join(f"{m['role']}: {m['content']}" for m in messages)
    summary = text.strip()

    log_flow("summary_generated_raw", {"preview": summary[:120]})
    return summary


# =========================
# SOCIAL / SMALL TALK
# =========================

SOCIAL_RESPONSES = {
    "vi": {
        "greeting": "ChÃ o báº¡n ğŸ‘‹",
        "thanks": "KhÃ´ng cÃ³ gÃ¬, ráº¥t vui Ä‘Æ°á»£c giÃºp báº¡n!",
        "goodbye": "Táº¡m biá»‡t nhÃ© ğŸ‘‹",
        "introduction": "MÃ¬nh lÃ  trá»£ lÃ½ AI há»— trá»£ khÃ¡ch hÃ ng, ráº¥t vui Ä‘Æ°á»£c há»— trá»£ báº¡n áº¡.",
        "chitchat": "ChÃ o báº¡n! MÃ¬nh cÃ³ thá»ƒ há»— trá»£ báº¡n vá» váº¥n Ä‘á» gÃ¬ hÃ´m nay?",
        "who_are_you": "MÃ¬nh lÃ  trá»£ lÃ½ AI há»— trá»£ khÃ¡ch hÃ ng cá»§a cÃ´ng ty áº¡. Ráº¥t vui Ä‘Æ°á»£c gáº·p báº¡n!",
        "what_doing": "MÃ¬nh Ä‘ang á»Ÿ Ä‘Ã¢y chá» há»— trá»£ báº¡n nÃ¨ ğŸ˜„ Báº¡n cáº§n giÃºp gÃ¬ hÃ´m nay?",
    },
    "en": {
        "greeting": "Hello ğŸ‘‹",
        "thanks": "You're welcome!",
        "goodbye": "Goodbye ğŸ‘‹",
        "introduction": "I'm an AI customer support assistant designed to help you.",
        "chitchat": "Hi there! How can I help you today?",
        "who_are_you": "I'm your AI customer support assistant. Nice to meet you!",
        "what_doing": "Just here waiting to assist you ğŸ˜„ What's on your mind?",
    },
    "zh": {
        "greeting": "ä½ å¥½ ğŸ‘‹",
        "thanks": "ä¸å®¢æ°”ï¼",
        "goodbye": "å†è§ ğŸ‘‹",
        "introduction": "æˆ‘æ˜¯AIå®¢æˆ·æ”¯æŒåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚",
        "chitchat": "ä½ å¥½ï¼ä»Šå¤©æˆ‘èƒ½å¸®æ‚¨ä»€ä¹ˆï¼Ÿ",
        "who_are_you": "æˆ‘æ˜¯æ‚¨çš„AIå®¢æˆ·æ”¯æŒåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´è®¤è¯†æ‚¨ï¼",
        "what_doing": "æˆ‘åœ¨è¿™é‡Œç­‰ç€å¸®æ‚¨å‘¢ ğŸ˜„ æ‚¨æœ‰ä»€ä¹ˆéœ€è¦ï¼Ÿ",
    },
    "ru": {
        "greeting": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ ğŸ‘‹",
        "thanks": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°!",
        "goodbye": "Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ ğŸ‘‹",
        "introduction": "Ğ¯ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ´ Ğ²Ğ°Ğ¼ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ.",
        "chitchat": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! Ğ§ĞµĞ¼ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ?",
        "who_are_you": "Ğ¯ Ğ²Ğ°Ñˆ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞŸÑ€Ğ¸ÑÑ‚Ğ½Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ğ¸Ñ‚ÑŒÑÑ!",
        "what_doing": "Ğ¯ Ğ·Ğ´ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ ğŸ˜„ Ğ Ñ‡ĞµĞ¼ Ğ´ÑƒĞ¼Ğ°ĞµÑ‚Ğµ?",
    },
}

SOCIAL_STARTERS = {
    "vi": [
        "hi", "hello", "hey", "xin chÃ o", "chÃ o", "chÃ o báº¡n", "chÃ o anh", "chÃ o chá»‹",
        "báº¡n lÃ  ai", "báº¡n tÃªn gÃ¬", "ai váº­y",
        "Ä‘ang lÃ m gÃ¬", "lÃ m gÃ¬ Ä‘áº¥y", "Ä‘ang lÃ m gÃ¬ tháº¿",
        "chÃ o buá»•i sÃ¡ng", "chÃ o buá»•i chiá»u", "chÃ o buá»•i tá»‘i",
    ],
    "en": [
        "hi", "hello", "hey",
        "who are you", "what's your name",
        "what are you doing", "how are you",
        "good morning", "good afternoon", "good evening",
    ],
    "zh": [
        "ä½ å¥½", "å—¨", "ä½ æ˜¯è°", "ä½ å«ä»€ä¹ˆåå­—",
        "ä½ åœ¨åšä»€ä¹ˆ", "ä½ å¥½å—",
    ],
    "ru": [
        "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚", "ĞºÑ‚Ğ¾ Ñ‚Ñ‹", "ĞºĞ°Ğº Ñ‚ĞµĞ±Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚",
        "Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑˆÑŒÑÑ", "ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°",
    ],
}

SMALL_TALK_PATTERNS = {
    "vi": {
        r"(báº¡n|em|mÃ¬nh).*(khá»e|á»•n|tháº¿ nÃ o)": "MÃ¬nh khá»e láº¯m áº¡, cáº£m Æ¡n báº¡n há»i! CÃ²n báº¡n thÃ¬ sao? ğŸ˜Š",
        r"(Ä‘ang lÃ m gÃ¬|Ä‘ang lÃ m)": "Äang chá» há»— trá»£ báº¡n Ä‘Ã¢y áº¡ ğŸ˜„ Báº¡n cáº§n giÃºp gÃ¬ nÃ o?",
    },
    "en": {
        r"(you).*(good|fine|how)": "I'm doing great, thanks! How about you? ğŸ˜Š",
        r"(what.*doing)": "Just here to help you out ğŸ˜„ What's up?",
    },
    "zh": {
        r"(ä½ ).*(å¥½|æ€ä¹ˆæ ·)": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼æ‚¨å‘¢ï¼Ÿ ğŸ˜Š",
        r"(åœ¨åšä»€ä¹ˆ)": "å°±åœ¨è¿™é‡Œå¸®æ‚¨ ğŸ˜„ æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ",
    },
    "ru": {
        r"(Ñ‚Ñ‹).*(Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾|ĞºĞ°Ğº)": "Ğ£ Ğ¼ĞµĞ½Ñ Ğ²ÑĞµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾! Ğ Ñƒ Ğ²Ğ°Ñ? ğŸ˜Š",
        r"(Ñ‡ĞµĞ¼.*Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑˆÑŒÑÑ)": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ´ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ ğŸ˜„ Ğ§Ñ‚Ğ¾ Ñƒ Ğ²Ğ°Ñ?",
    },
}

DENY_KEYWORDS = {
    "vi": ["sai rá»“i", "khÃ´ng Ä‘Ãºng", "khÃ´ng pháº£i", "tÃ´i khÃ´ng muá»‘n", "khÃ´ng pháº£i váº­y", "láº¡i sai"],
    "en": ["not correct", "wrong", "that's wrong", "incorrect", "not right", "not what i want", "nope", "that's not it"],
    "zh": ["ä¸å¯¹", "é”™äº†", "ä¸æ˜¯è¿™æ ·"],
    "ru": ["Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾", "Ğ½Ğµ Ñ‚Ğ¾", "Ğ¾ÑˆĞ¸Ğ±ĞºĞ°"],
}

PRODUCT_KEYWORDS = {
    "vi": [
        "hidemium", "api hidemium", "hidemium api", "hidemium lÃ  gÃ¬",
        "dá»‹ch vá»¥ hidemium", "hidemium proxy", "áº©n danh hidemium",
    ],
    "en": [
        "hidemium", "hidemium api", "what is hidemium",
        "hidemium proxy", "tell me about hidemium",
    ],
    "zh": ["hidemium", "hidemium api"],
    "ru": ["hidemium", "hidemium api"],
}


def detect_language(text: str) -> str:
    if re.search(r'[\u4e00-\u9fff]', text):
        return "zh"
    if re.search(r'[\u0400-\u04ff]', text):
        return "ru"
    try:
        text.encode("ascii")
        return "en"
    except UnicodeEncodeError:
        return "vi"



def translate_to_vi(text: str, src_lang: str) -> str:
    if src_lang == "vi":
        return text
    return text

def detect_anchor_reference(message: str) -> Optional[int]:
    msg = message.lower()

    # match: "cÃ¢u 2", "cÃ¢u há»i 3", "quay láº¡i cÃ¢u 4"
    m = re.search(r"cÃ¢u(?:\s+há»i)?\s*(\d+)", msg)
    if m:
        return int(m.group(1)) - 1

    return None



def translate_from_vi(text: str, target_lang: str) -> str:
    if target_lang == "vi":
        return text
    return text


def is_valid_chunk(text: str) -> bool:
    text = text.strip()
    if not text or text in {"--", "-", "..."}:
        return False
    if len(text) < 15:
        return False
    return True


def build_answer_from_chunks(
    docs: List[str],
    query: Optional[str] = None,
    max_chars: int = 800
) -> str:
    valid_docs = []
    for d in docs:
        if not d:
            continue
        d = d.strip()
        if len(d) < 15:
            continue
        if d in {"--", "-", "...", "---"}:
            continue
        valid_docs.append(d)

    if not valid_docs:
        return ""

    best_docs = valid_docs

    if query:
        q = query.lower()
        q = re.sub(r"[^\w\sÃ€-á»¹]", " ", q)
        q = re.sub(r"\s+", " ", q).strip()

        tokens = [t for t in q.split() if len(t) > 2]

        def soft_match(doc: str) -> bool:
            dl = doc.lower()
            hit = sum(1 for t in tokens if t in dl)
            return hit >= max(1, len(tokens) // 3)

        matched = [d for d in valid_docs if soft_match(d)]
        if matched:
            best_docs = matched

    text = best_docs[0]

    match = re.search(r"\*\*?A:\*\*?\s*(.+)", text, re.DOTALL | re.IGNORECASE)
    extracted = match.group(1).strip() if match else text.strip()

    extracted = re.sub(r"^#+\s*", "", extracted)
    extracted = re.sub(r"(?m)^\s*#+\s*", "", extracted)
    extracted = re.sub(r"###\s*Má»¤C:.*", "", extracted, flags=re.IGNORECASE)
    extracted = re.sub(r"Má»¤C:\s*[^\n]+", "", extracted, flags=re.IGNORECASE)
    extracted = re.sub(r"\*\*?Q:\*\*?\s*.*", "", extracted, flags=re.IGNORECASE)
    extracted = re.sub(r"\*\*(.*?)\*\*", r"\1", extracted)
    extracted = re.sub(r"(?m)^\s*-{3,}\s*$", "", extracted)
    extracted = re.sub(r"(?m)^\s*-\s*", "â€¢ ", extracted)
    extracted = re.sub(r"\n{3,}", "\n\n", extracted)
    extracted = extracted.strip()

    return extracted[:max_chars].strip()


def wrap_cskh_answer(answer: str, lang: str) -> str:
    if not answer:
        return answer

    suffixes = {
        "vi": "Báº¡n cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡? ğŸ˜Š",
        "en": "Anything else I can help with? ğŸ˜Š",
        "zh": "è¿˜æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®æ‚¨çš„å—ï¼Ÿ ğŸ˜Š",
        "ru": "Ğ§ĞµĞ¼ ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ? ğŸ˜Š",
    }

    return f"{answer} {suffixes.get(lang, suffixes['en'])}"


def build_alternative_answer(docs: List[str], lang: str) -> str:
    prefixes = {
        "vi": "CÃ³ thá»ƒ mÃ¬nh Ä‘Ã£ hiá»ƒu chÆ°a Ä‘Ãºng trÆ°á»ng há»£p cá»§a báº¡n.\nTrong tÃ i liá»‡u hiá»‡n cÃ³, mÃ¬nh tháº¥y cÃ¡c thÃ´ng tin sau:\n",
        "en": "Maybe I didn't understand your case correctly.\nIn the current documentation, I found the following:\n",
        "zh": "å¯èƒ½æˆ‘æ²¡å®Œå…¨ç†è§£æ‚¨çš„æƒ…å†µã€‚\nåœ¨ç°æœ‰æ–‡æ¡£ä¸­ï¼Œæˆ‘æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n",
        "ru": "Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ñ Ğ½Ğµ ÑĞ¾Ğ²ÑĞµĞ¼ Ğ¿Ğ¾Ğ½ÑĞ» Ğ²Ğ°Ñˆ ÑĞ»ÑƒÑ‡Ğ°Ğ¹.\nĞ’ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½Ğ°ÑˆĞµĞ» ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ:\n",
    }

    text = prefixes.get(lang, prefixes["en"])

    for i, d in enumerate(docs[:3]):
        summary_vi = build_answer_from_chunks([d])
        summary = translate_from_vi(summary_vi, lang)
        label = chr(65 + i)
        text += f"â€¢ TrÆ°á»ng há»£p {label}: {summary}\n" if lang == "vi" else f"â€¢ Case {label}: {summary}\n"

    questions = {
        "vi": "\nBáº¡n Ä‘ang quan tÃ¢m trÆ°á»ng há»£p nÃ o Ä‘á»ƒ mÃ¬nh há»— trá»£ chÃ­nh xÃ¡c hÆ¡n nhÃ©?",
        "en": "\nWhich case are you referring to so I can assist more accurately?",
        "zh": "\næ‚¨å…³å¿ƒå“ªä¸ªæƒ…å†µï¼Œè®©æˆ‘æ›´å‡†ç¡®åœ°å¸®åŠ©æ‚¨ï¼Ÿ",
        "ru": "\nĞšĞ°ĞºĞ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ Ğ¼Ğ¾Ğ³ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ?",
    }

    text += questions.get(lang, questions["en"])
    return text


# =========================
# CHAT SERVICE
# =========================

class ChatService:

    def __init__(self):
        self.vector = VectorStoreManager()
        self.quick_reply = QuickReplyHandler()

    async def process_chat_message(
        self,
        message: str,
        session_id: str = "default"
    ) -> Dict[str, Any]:

        pipeline_logger.info("=" * 80)
        pipeline_logger.info(f"[INPUT] {message}")

        message = message.strip()
        if not message:
            return {"response": "Please say something ğŸ˜…"}

        # =========================
        # LOAD SESSION + SUMMARY
        # =========================
        session = SESSION_MEMORY.setdefault(session_id, {})

        user_id = session_id  # táº¡m thá»i dÃ¹ng session_id

        if "summary" not in session:
            old_summary = load_latest_summary(user_id,session_id)
            session["summary"] = old_summary or ""
            if old_summary:
                log_flow("summary_loaded", {"preview": old_summary[:120]})

        save_message("user", message, session_id=session_id)

        # =========================
        # AUTO SUMMARY TRIGGER
        # =========================
        messages = load_messages(session_id, limit=SUMMARY_MIN_TURNS * 2)
        if len(messages) >= SUMMARY_MIN_TURNS * 2:
            summary = await summarize_session(session_id)
            if summary:
                save_conversation_summary(user_id,session_id, summary)
                session["summary"] = summary
                log_flow("summary_saved", {"preview": summary[:120]})

        user_lang = detect_language(message)

        support_state = session.setdefault("support_state", {
            "phase": "idle",
            "last_query": None,
            "last_answer": None,
            "deny_count": 0,
            "language": user_lang,
            "escalated": False,
            "summary": session.get("summary", ""),
            "query_history": [],
            "context_anchors": {}
        })

        support_state["language"] = user_lang
        support_state["summary"] = session.get("summary", "")

        msg_lc = message.lower()

        deny_list = DENY_KEYWORDS.get(user_lang, [])
        if support_state["deny_count"] > 0 and not any(x in msg_lc for x in deny_list):
            support_state["deny_count"] = 0

        # 1) DENY
        if any(x in msg_lc for x in deny_list):
            resp = await self.handle_deny(support_state)
            if support_state["deny_count"] >= 3:
                support_state["escalated"] = True
            save_message("bot", resp, session_id)
            log_flow("handling_deny", {"deny_count": support_state["deny_count"]})
            return {"response": resp, "mode": "cskh_deny"}

        # 2) PRODUCT PRIORITY
        product_keywords_all = PRODUCT_KEYWORDS.get(user_lang, []) + PRODUCT_KEYWORDS.get("en", [])
        if any(kw in msg_lc for kw in product_keywords_all) and support_state["deny_count"] == 0:
            log_flow("route_product_inquiry", {"query": message, "lang": user_lang})
            return await self.handle_knowledge_flow(message, support_state, session_id)

        # 3) SMALL TALK
        for pattern, reply in SMALL_TALK_PATTERNS.get(user_lang, {}).items():
            if re.search(pattern, msg_lc):
                save_message("bot", reply, session_id)
                log_flow("small_talk_hit", {"pattern": pattern})
                return {"response": reply, "mode": "small_talk"}

        # 4) SOCIAL
        starters = SOCIAL_STARTERS.get(user_lang, [])
        for x in starters:
            pattern = r"\b" + re.escape(x) + r"\b"
            if re.search(pattern, msg_lc):
                responses = SOCIAL_RESPONSES.get(user_lang, SOCIAL_RESPONSES["en"])
                if any(y in msg_lc for y in ["lÃ  ai", "who are you"]):
                    answer = responses["who_are_you"]
                elif any(y in msg_lc for y in ["lÃ m gÃ¬", "doing"]):
                    answer = responses["what_doing"]
                else:
                    answer = responses["chitchat"]

                save_message("bot", answer, session_id)
                log_flow("route_cskh_social_hard", {"answer": answer})
                return {"response": answer, "mode": "social"}

        # 5) BAD WORD
        if contains_swear(message):
            resp = get_swear_response()
            save_message("bot", resp, session_id)
            return {"response": resp}

        # 6) QUICK REPLY
        if len(message) <= 8:
            qr = self.quick_reply.get_quick_response(message)
            if qr:
                save_message("bot", qr, session_id)
                return {"response": qr, "mode": "quick_reply"}

        # 7) GEMINI FALLBACK CLASSIFIER
        intent_type = "knowledge"
        intent = "unknown"

        log_flow("intent_llm_probe", {"message": message})

        try:
            analysis = await asyncio.to_thread(analyze_question, message)
            intent_type = analysis.get("type", "knowledge")
            intent = analysis.get("intent", "unknown")
            log_flow("intent_detected", {"type": intent_type, "intent": intent})
        except Exception as e:
            log_flow("intent_llm_error", {"error": str(e)})

        # 8) SOCIAL FALLBACK
        if intent_type == "social":
            responses = SOCIAL_RESPONSES.get(user_lang, SOCIAL_RESPONSES["en"])
            answer = responses.get("chitchat", responses["chitchat"])
            save_message("bot", answer, session_id)
            return {"response": answer, "mode": "social"}

        # 9) ACTION INTENT
        handler_cls = intent_registry.get(intent_type, intent)
        if handler_cls:
            handler = handler_cls()
            resp = await handler.handle(session)
            return resp

        # 10) DEFAULT KNOWLEDGE
        return await self.handle_knowledge_flow(message, support_state, session_id)

    async def handle_knowledge_flow(
        self,
        message: str,
        support_state: Dict[str, Any],
        session_id: str
    ) -> Dict[str, Any]:

        support_state["phase"] = "answering"

        user_lang = support_state["language"]
        query_vi = translate_to_vi(message, user_lang)
        anchor_idx = detect_anchor_reference(message)
        log_flow("anchor_detected", {
            "raw_message": message,
            "anchor_idx": anchor_idx
        })

        if anchor_idx is not None:
            anchors = support_state.get("context_anchors", {})
            base_ctx = anchors.get(anchor_idx)

            if base_ctx:
                query_vi = f"{base_ctx}\n{query_vi}"
                log_flow("anchor_context_injected", {
                    "anchor_idx": anchor_idx,
                    "anchor_preview": base_ctx[:120]
                })

# =========================
# INJECT SUMMARY CONTEXT (SAFE)
# =========================
        summary_ctx = support_state.get("summary") or ""

        def same_topic(summary: str, query: str) -> bool:
            s = summary.lower()
            q = query.lower()

            # rule tá»‘i thiá»ƒu: cÃ¹ng nháº¯c Hidemium thÃ¬ má»›i inject
            if "hidemium" in s and "hidemium" in q:
                return True

            # sau nÃ y má»Ÿ rá»™ng thÃªm topic khÃ¡c á»Ÿ Ä‘Ã¢y
            return False

        if summary_ctx and same_topic(summary_ctx, query_vi):
            query_vi = f"{summary_ctx}\n{query_vi}"
            log_flow("summary_injected", {"len": len(summary_ctx)})
        else:
            if summary_ctx:
                log_flow("summary_skipped_topic_mismatch", {
                    "summary_preview": summary_ctx[:120],
                    "query": query_vi
                })


        # =========================
        # NORMALIZE QUERY
        # =========================
        query_vi = re.sub(r"[^\w\sÃ€-á»¹]", " ", query_vi)
        query_vi = re.sub(r"\s+", " ", query_vi).strip()
        log_flow("query_normalized", {
            "query_vi": query_vi
        })

        qh = support_state.setdefault("query_history", [])
        anchors = support_state.setdefault("context_anchors", {})
        if anchor_idx is None:
            qh.append(query_vi)
            idx = len(qh) - 1
            anchors[idx] = query_vi
            log_flow("anchor_saved", {
                "idx": idx,
                "query_preview": query_vi[:120]
            })
        else:
            log_flow("anchor_skipped_save", {
                "anchor_idx": anchor_idx,
                "query_preview": query_vi[:120]
            })

        if support_state.get("last_query") and support_state["last_query"] != query_vi:
            support_state["deny_count"] = 0
            log_flow("deny_reset_check", {
            "last_query": support_state.get("last_query"),
            "current_query": query_vi,
            "deny_count_after": support_state["deny_count"]
        })

        # náº¿u user Ä‘ang quay láº¡i cÃ¢u cÅ© â†’ last_query = base_ctx
        if anchor_idx is not None and base_ctx:
            support_state["last_query"] = base_ctx
            log_flow("last_query_set_from_anchor", {
                "last_query_preview": base_ctx[:120]
            })
        else:
            support_state["last_query"] = query_vi
            log_flow("last_query_set_from_current", {
                    "last_query_preview": query_vi[:120]
                })


        docs = []
        metas = []

        if "hidemium" in query_vi.lower():
            expanded_queries = [
                query_vi,
                "Hidemium API lÃ  gÃ¬",
                "Hidemium lÃ  gÃ¬",
                "dá»‹ch vá»¥ Hidemium API",
                "tÃ­nh nÄƒng Hidemium API",
                "cÃ¡ch sá»­ dá»¥ng Hidemium",
            ]

            all_docs = []
            all_metas = []

            for q in expanded_queries:
                d, m, _ = self.vector.query_documents(
                    query=q, user_id=session_id, n_results=15
                )
                all_docs.extend(d)
                all_metas.extend(m)

            seen = set()
            docs = []
            metas = []

            for d, m in zip(all_docs, all_metas):
                key = d[:120]
                if key in seen:
                    continue
                seen.add(key)
                docs.append(d)
                metas.append(m)

            docs = docs[:40]
            metas = metas[:40]

            log_flow("query_expansion", {
                "original": query_vi,
                "expanded_count": len(expanded_queries)
            })
        else:
            docs, metas, _ = self.vector.query_documents(
                query=query_vi, user_id=session_id, n_results=20
            )

        log_flow("rag_docs_debug", {
            "query": query_vi,
            "doc_count": len(docs),
            "sources": list({
                m.get("source") for m in metas if m and m.get("source")
            }),
            "doc_previews": [d[:120] for d in docs[:5]]
        })

        answer_vi = build_answer_from_chunks(docs, query_vi)

        if not answer_vi or len(answer_vi.strip()) < 30:
            if "hidemium" in query_vi.lower():
                answer_vi = (
                    "Hidemium API lÃ  bá»™ API cho phÃ©p báº¡n táº¡o, quáº£n lÃ½ vÃ  khá»Ÿi cháº¡y "
                    "cÃ¡c browser profile Hidemium tá»« tool bÃªn ngoÃ i. "
                    "API thÆ°á»ng dÃ¹ng Ä‘á»ƒ tÃ­ch há»£p vá»›i Puppeteer, Playwright "
                    "hoáº·c automation framework riÃªng.\n\n"
                    "Báº¡n Ä‘ang muá»‘n:\n"
                    "â€¢ Äiá»u khiá»ƒn profile qua API?\n"
                    "â€¢ Káº¿t ná»‘i vá»›i Puppeteer/Playwright?\n"
                    "â€¢ Hay build tool riÃªng dÃ¹ng profile Hidemium?"
                )
            else:
                answer_vi = (
                    "MÃ¬nh chÆ°a tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong tÃ i liá»‡u hiá»‡n táº¡i. "
                    "Báº¡n cÃ³ thá»ƒ mÃ´ táº£ chi tiáº¿t hÆ¡n Ä‘Æ°á»£c khÃ´ng áº¡? ğŸ˜Š"
                )

        support_state["last_answer"] = answer_vi

        answer = answer_vi
        if user_lang != "vi":
            log_flow("translate_output_llm", {"lang": user_lang})
            answer = await asyncio.to_thread(
                translate_text, answer_vi, user_lang
            )

        answer = wrap_cskh_answer(answer, user_lang)

        log_flow("rag_response", {"answer_preview": answer[:150]})
        save_message("bot", answer, session_id)

        return {"response": answer, "mode": "knowledge"}

    async def handle_deny(self, support_state: Dict[str, Any]) -> str:

        support_state["phase"] = "handling_deny"
        support_state["deny_count"] += 1

        last_query_vi = support_state.get("last_query")
        log_flow("deny_using_last_query", {
            "last_query_preview": (last_query_vi or "")[:120],
            "deny_count": support_state["deny_count"]
        })

        lang = support_state["language"]

        if support_state["deny_count"] >= 3:
            escalations = {
                "vi": (
                    "MÃ¬nh xin lá»—i vÃ¬ chÆ°a há»— trá»£ Ä‘Ãºng. "
                    "MÃ¬nh sáº½ chuyá»ƒn báº¡n sang bá»™ pháº­n CSKH nhÃ©. "
                    "Báº¡n Ä‘á»ƒ láº¡i thÃ´ng tin Ä‘á»ƒ bÃªn mÃ¬nh liÃªn há»‡ há»— trá»£ áº¡ ğŸ˜Š"
                ),
                "en": (
                    "Sorry I couldn't get it right yet. "
                    "I'll escalate to our support team. "
                    "Please leave your details ğŸ˜Š"
                ),
                "zh": (
                    "å¾ˆæŠ±æ­‰ç›®å‰è¿˜æ²¡èƒ½æ­£ç¡®å¸®åŠ©æ‚¨ã€‚"
                    "æˆ‘ä¼šå°†æ‚¨çš„é—®é¢˜è½¬äº¤ç»™æˆ‘ä»¬çš„æ”¯æŒå›¢é˜Ÿã€‚"
                    "è¯·ç•™ä¸‹æ‚¨çš„è”ç³»æ–¹å¼ ğŸ˜Š"
                ),
                "ru": (
                    "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¼Ğ½Ğµ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾. "
                    "Ğ¯ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ¼ Ğ²Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ°ÑˆĞµĞ¹ ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸. "
                    "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ ÑĞ²Ğ¾Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ğŸ˜Š"
                ),
            }
            return escalations.get(lang, escalations["en"])

        if support_state["deny_count"] == 2 and last_query_vi:
            rephrased = last_query_vi + " cÃ¡c trÆ°á»ng há»£p"
            docs, _, _ = self.vector.query_documents(
                query=rephrased, user_id="deny", n_results=50
                
            )
            
            
            

            if not docs:
                hard_cases = {
                    "vi": [
                        "Äiá»u khiá»ƒn profile tá»« tool khÃ¡c qua API",
                        "Káº¿t ná»‘i profile vá»›i Puppeteer / Playwright",
                        "XÃ¢y tool riÃªng Ä‘á»ƒ quáº£n lÃ½ profile",
                    ],
                    "en": [
                        "Control profile from another tool via API",
                        "Connect profile with Puppeteer / Playwright",
                        "Build your own tool to manage profiles",
                    ],
                    "zh": [
                        "é€šè¿‡ API ä»å…¶ä»–å·¥å…·æ§åˆ¶é…ç½®æ–‡ä»¶",
                        "å°†é…ç½®æ–‡ä»¶è¿æ¥åˆ° Puppeteer / Playwright",
                        "æ„å»ºè‡ªå·±çš„å·¥å…·æ¥ç®¡ç†é…ç½®æ–‡ä»¶",
                    ],
                    "ru": [
                        "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¼ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· API",
                        "ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ Ğº Puppeteer / Playwright",
                        "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑĞ¼Ğ¸",
                    ],
                }

                cases = hard_cases.get(lang, hard_cases["en"])

                intro = {
                    "vi": "CÃ³ thá»ƒ báº¡n Ä‘ang nÃ³i tá»›i má»™t trong cÃ¡c trÆ°á»ng há»£p sau:\n",
                    "en": "You might be referring to one of these cases:\n",
                    "zh": "æ‚¨å¯èƒ½æŒ‡çš„æ˜¯ä»¥ä¸‹æƒ…å†µä¹‹ä¸€ï¼š\n",
                    "ru": "Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ğ²Ñ‹ Ğ¸Ğ¼ĞµĞµÑ‚Ğµ Ğ² Ğ²Ğ¸Ğ´Ñƒ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²:\n",
                }.get(lang, "You might be referring to one of these cases:\n")

                text = intro

                for i, c in enumerate(cases):
                    label = chr(65 + i)
                    if lang == "vi":
                        text += f"â€¢ TrÆ°á»ng há»£p {label}: {c}\n"
                    else:
                        text += f"â€¢ Case {label}: {c}\n"

                question = {
                    "vi": "\nBáº¡n Ä‘ang quan tÃ¢m hÆ°á»›ng tráº£ lá»i nÃ o?",
                    "en": "\nWhich case are you referring to?",
                    "zh": "\næ‚¨å…³å¿ƒçš„æ˜¯å“ªä¸€ç§æƒ…å†µï¼Ÿ",
                    "ru": "\nĞšĞ°ĞºĞ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒĞµÑ‚?",
                }.get(lang, "\nWhich case are you referring to?")

                return text + question

            return build_alternative_answer(docs, lang)

        if not last_query_vi:
            return {
                "vi": "MÃ¬nh chÆ°a rÃµ báº¡n Ä‘ang phá»§ Ä‘á»‹nh pháº§n nÃ o. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng áº¡? ğŸ˜Š",
                "en": "I'm not sure what part you're disagreeing with. Could you clarify? ğŸ˜Š",
                "zh": "æˆ‘ä¸å¤ªç¡®å®šæ‚¨ä¸åŒæ„å“ªä¸€éƒ¨åˆ†ã€‚æ‚¨èƒ½å†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ ğŸ˜Š",
                "ru": "Ğ¯ Ğ½Ğµ ÑĞ¾Ğ²ÑĞµĞ¼ Ğ¿Ğ¾Ğ½ÑĞ», Ñ Ñ‡ĞµĞ¼ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹ Ğ½Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ñ‹. ĞĞµ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ±Ñ‹ Ğ²Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ? ğŸ˜Š",
            }.get(lang, "I'm not sure what part you're disagreeing with. Could you clarify? ğŸ˜Š")

        return {
            "vi": "CÃ³ thá»ƒ mÃ¬nh chÆ°a hiá»ƒu Ä‘Ãºng. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng áº¡? ğŸ˜Š",
            "en": "Maybe I misunderstood. Could you clarify? ğŸ˜Š",
            "zh": "å¯èƒ½æˆ‘æ²¡æœ‰ç†è§£æ¸…æ¥šã€‚æ‚¨èƒ½å†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ ğŸ˜Š",
            "ru": "Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ½ÑĞ». ĞĞµ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ±Ñ‹ Ğ²Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ? ğŸ˜Š",
        }.get(lang, "Maybe I misunderstood. Could you clarify? ğŸ˜Š")


_chat_service = ChatService()


async def process_chat_message(message: str, session_id: str = "default"):
    return await _chat_service.process_chat_message(message, session_id)
