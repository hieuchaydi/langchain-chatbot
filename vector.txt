# vector_store.py – BẢN CUỐI CÙNG, CHẠY MƯỢT TRÊN WINDOWS 2025
# Tốc độ: 1.0–1.6s/query | Không crash reload | Không lỗi vàng

import chromadb
from chromadb.utils import embedding_functions
from pathlib import Path
from rank_bm25 import BM25Okapi
from typing import List, Tuple
import re
from functools import lru_cache
import time

# ===================================================================
# 1. CHROMA SETUP
# ===================================================================
ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
client = chromadb.PersistentClient(path="./vectorstore")

def get_collection():
    return client.get_or_create_collection(
        name="markdown_docs",
        embedding_function=ef,
        metadata={"hnsw:space": "cosine"}
    )

# ===================================================================
# 2. BM25 – LAZY REBUILD THÔNG MINH (chỉ rebuild khi thực sự thay đổi)
# ===================================================================
_bm25: BM25Okapi | None = None
_all_docs_tokenized: List[List[str]] = []
_all_ids: List[str] = []
_last_doc_count = -1  # để phát hiện thay đổi

def _rebuild_bm25_if_needed():
    global _bm25, _all_docs_tokenized, _all_ids, _last_doc_count
    coll = get_collection()
    current_count = coll.count()

    # Nếu số lượng tài liệu không đổi → bỏ qua rebuild
    if current_count == _last_doc_count:
        return

    # Lấy cả documents + ids (bắt buộc phải kèm documents hoặc metadatas)
    data = coll.get(include=["documents"])
    docs = data.get("documents", [])
    ids = data.get("ids", [])

    _all_docs_tokenized = [doc.lower().split() for doc in docs]
    _all_ids = ids
    _bm25 = BM25Okapi(_all_docs_tokenized) if _all_docs_tokenized else None
    _last_doc_count = current_count
    print(f"BM25 rebuilt – {len(docs)} docs")

# Khởi động lần đầu
_rebuild_bm25_if_needed()

# ===================================================================
# 3. ADD DOCUMENTS – NHANH + CHUẨN
# ===================================================================
def add_documents(file_content: str, filename: str) -> None:
    coll = get_collection()
    filename_clean = Path(filename).name

    # Xóa file cũ
    coll.delete(where={"source": filename_clean})

    lines = file_content.split("\n")
    chunks = []
    current_chunk = []
    current_title = "Không có tiêu đề"

    for line in lines:
        line = line.rstrip()
        if line.startswith("#"):
            hash_count = len(line) - len(line.lstrip("#"))
            if 1 <= hash_count <= 4:
                if current_chunk:
                    text = "\n".join(current_chunk).strip()
                    if text:
                        chunks.append({"text": text, "title": current_title})
                current_title = line.lstrip("# ").strip() or f"Heading cấp {hash_count}"
                current_chunk = [line]
                continue
        current_chunk.append(line)

    if current_chunk:
        text = "\n".join(current_chunk).strip()
        if text:
            chunks.append({"text": text, "title": current_title})

    # Fallback nếu không có heading
    if not chunks:
        words = file_content.split()
        for i in range(0, len(words), 500):
            chunk = words[i:i+600]
            if chunk:
                chunks.append({"text": " ".join(chunk), "title": "Không có tiêu đề"})

    if chunks:
        docs = [c["text"] for c in chunks]
        metadatas = [{
            "source": filename_clean,
            "title": c["title"],
            "title_clean": re.sub(r'\s+', ' ', re.sub(r'[^\w\s]', '', c["title"].lower())).strip()
        } for c in chunks]
        ids = [f"{filename_clean}_{i}" for i in range(len(chunks))]
        coll.add(documents=docs, metadatas=metadatas, ids=ids)
        print(f"Added {len(chunks)} chunks from {filename_clean}")

    # Chỉ rebuild khi thực sự cần
    _rebuild_bm25_if_needed()

# ===================================================================
# 4. HYBRID SEARCH SIÊU NHANH + CACHE
# ===================================================================
def _hybrid_search_raw(query: str, n_results: int) -> Tuple[List[str], List[dict]]:
    coll = get_collection()
    vec = coll.query(
        query_texts=[query],
        n_results=n_results + 6,  # lấy dư một chút để gộp
        include=["documents", "metadatas", "distances"]
    )

    v_docs = vec["documents"][0] or []
    v_metas = vec["metadatas"][0] or []
    v_dist = vec["distances"][0] or []
    v_ids = vec["ids"][0] or []

    if not _bm25 or not _all_ids:
        return v_docs[:n_results], v_metas[:n_results]

    bm25_scores = _bm25.get_scores(query.lower().split())
    scores = {doc_id: bm25_scores[i] for i, doc_id in enumerate(_all_ids)}
    for vid, dist in zip(v_ids, v_dist):
        scores[vid] = scores.get(vid, 0) + (1.0 - dist) * 20

    top_ids = [x[0] for x in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:n_results]]
    result = coll.get(ids=top_ids, include=["documents", "metadatas"])
    return result.get("documents", []), result.get("metadatas", [])

@lru_cache(maxsize=100)
def query_documents(query: str, n_results: int = 8) -> Tuple[List[str], List[dict]]:
    start = time.time()
    docs, metas = _hybrid_search_raw(query.lower(), n_results)
    elapsed = time.time() - start
    cache_info = query_documents.cache_info()
    print(f"QUERY | {elapsed:.3f}s | {len(docs)} results | Cache: {cache_info.hits}/{cache_info.hits + cache_info.misses}")
    return docs, metas

# ===================================================================
# 5. HỖ TRỢ
# ===================================================================
def reset_vectorstore():
    client.delete_collection("markdown_docs")
    global _bm25, _all_docs_tokenized, _all_ids, _last_doc_count
    _bm25 = None
    _all_docs_tokenized = []
    _all_ids = []
    _last_doc_count = -1
    print("Vector store đã được reset!")


def get_all_documents():
    """Lấy toàn bộ tài liệu (dùng để debug hoặc hiển thị danh sách)"""
    coll = get_collection()
    data = coll.get(include=["documents", "metadatas"])
    if "ids" not in data or not data["ids"]:
        data["ids"] = [f"unknown_{i}" for i in range(len(data["documents"]))]
    return data
# Warm nhẹ khi khởi động
if get_collection().count() > 0:
    query_documents("startup warm", 1)
print("Vectorstore ready – tốc độ ánh sáng!")